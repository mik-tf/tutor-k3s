---
# Deploy role - replaces deploy-openedx.sh

- name: Check if kubectl is available
  ansible.builtin.command: which kubectl
  register: kubectl_check
  ignore_errors: true
  changed_when: false

- name: Fail if kubectl is not installed
  fail:
    msg: "Error: kubectl is not installed or not in PATH"
  when: kubectl_check.rc != 0

- name: Check if tutor is available
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor --version"
  register: tutor_check
  ignore_errors: true
  changed_when: false

- name: Include API health check
  include_tasks: api_health_check.yml

- name: Fail if tutor is not installed
  fail:
    msg: "Error: tutor is not installed or not in PATH. Please make sure you've run the setup role first."
  when: tutor_check.rc != 0

# Kubeconfig already defined in group_vars/all.yml

- name: Wait for K3s API server to be ready
  ansible.builtin.shell: |
    echo "Checking if K3s API server is ready..."
    for i in $(seq 1 18); do
      if kubectl get nodes &>/dev/null; then
        echo "API server is ready!"
        exit 0
      fi
      echo "Attempt $i/18: API server not ready yet, waiting 10 seconds..."
      sleep 10
    done
    echo "API server still not ready after 3 minutes."
    exit 1
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
  register: k8s_api_check
  ignore_errors: true
  changed_when: false

- name: Display API server status
  debug:
    msg: |
      K3s API Server Status: {{ "READY" if k8s_api_check.rc == 0 else "NOT READY" }}
      
      The API server must be ready before deployment can proceed.
      If not ready, please check your K3s cluster:
      
      ssh root@<node-ip> "systemctl status k3s"
      
      You may need to wait longer for initialization or restart K3s:
      
      ssh root@<node-ip> "systemctl restart k3s"
  
- name: Fail if API server is not ready after maximum retries
  fail:
    msg: "Kubernetes API server is not ready after 3 minutes of retries. Please check your K3s cluster and retry."
  when: k8s_api_check.rc != 0

- name: Check file descriptor limits
  ansible.builtin.shell: ulimit -n
  register: current_limit
  args:
    executable: /bin/bash
  changed_when: false

- name: Display current file descriptor limit
  debug:
    msg: "Current file descriptor limit: {{ current_limit.stdout }}"

- name: Try to increase file descriptor limit for current session
  ansible.builtin.shell: ulimit -n {{ min_file_descriptors }} 2>/dev/null || echo "Cannot increase limit"
  register: increase_limit
  args:
    executable: /bin/bash
  when: current_limit.stdout | int < min_file_descriptors | int
  ignore_errors: true
  changed_when: false

- name: Check if limit was increased
  ansible.builtin.shell: ulimit -n
  register: new_limit
  args:
    executable: /bin/bash
  when: current_limit.stdout | int < min_file_descriptors | int
  changed_when: false

- name: Display warning if file descriptor limits are too low
  debug:
    msg: |
      Warning: File descriptor limit is too low ({{ current_limit.stdout }}).
      OpenEdX container images require higher limits to prevent 'too many open files' errors.
      You may encounter 'ImagePullBackOff' errors with 'too many open files' messages.
      To fix this permanently, run the prepare role or manually increase system limits.
  when: current_limit.stdout | int < min_file_descriptors | int and new_limit.stdout | int <= current_limit.stdout | int

- name: Clean up any existing failed deployment
  ansible.builtin.shell: |
    kubectl delete jobs --all -n {{ openedx_namespace }} 2>/dev/null || true
    kubectl delete configmaps --all -n {{ openedx_namespace }} 2>/dev/null || true
  args:
    executable: /bin/bash
  ignore_errors: true
  changed_when: false

- name: Create openedx namespace if it doesn't exist
  ansible.builtin.shell: |
    kubectl create namespace {{ openedx_namespace }} --dry-run=client -o yaml | kubectl apply -f - 
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
  register: namespace_result
  retries: 5
  delay: 10
  until: namespace_result.rc == 0 or "AlreadyExists" in namespace_result.stderr
  changed_when: "'created' in namespace_result.stdout | default('')"

- name: Check if plugins are enabled
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor plugins list"
  register: plugins_list
  changed_when: false

- name: Install latest recommended Tutor plugins
  ansible.builtin.pip:
    name:
      - tutor-mfe
      - tutor-indigo
      - tutor-discovery
      - tutor-ecommerce
    state: latest
    virtualenv: "{{ venv_dir }}"

- name: Enable indigo plugin if needed
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor plugins enable indigo"
  changed_when: true
  when: "'indigo.*enabled' not in plugins_list.stdout"

- name: Enable mfe plugin if needed
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor plugins enable mfe"
  changed_when: true
  when: "'mfe.*enabled' not in plugins_list.stdout"

- name: Enable discovery plugin if needed
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor plugins enable discovery"
  changed_when: true
  when: "'discovery.*enabled' not in plugins_list.stdout"

- name: Enable ecommerce plugin if needed
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor plugins enable ecommerce"
  changed_when: true
  when: "'ecommerce.*enabled' not in plugins_list.stdout"

- name: Check if Docker registry authentication is configured
  ansible.builtin.shell: |
    kubectl get secret dockerhub-creds -n {{ openedx_namespace }} -o name || echo "not found"
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
  register: registry_auth
  retries: 3
  delay: 5
  until: registry_auth.rc == 0
  changed_when: false
  failed_when: false

- name: Configure Docker registry authentication if needed
  include_role:
    name: registry
  when: registry_auth.stdout == "not found"

- name: Check for Longhorn storage classes
  ansible.builtin.shell: |
    kubectl get storageclass -o json | jq -r '.items[] | select(.provisioner == "driver.longhorn.io") | .metadata.name'
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
  register: longhorn_sc
  changed_when: false
  ignore_errors: true

- name: Display Longhorn storage classes if available
  debug:
    msg: "Found Longhorn storage classes: {{ longhorn_sc.stdout_lines }}"
  when: longhorn_sc.rc == 0 and longhorn_sc.stdout_lines | length > 0

- name: Configure Tutor to use Longhorn storage
  block:
    - name: Check for DB-optimized storage class
      ansible.builtin.shell: |
        kubectl get storageclass longhorn-db-performance 2>/dev/null || echo "not found"
      environment:
        KUBECONFIG: "{{ kubeconfig }}"
      register: db_sc_check
      changed_when: false
      
    - name: Set default storage class for all PVCs
      ansible.builtin.command: "{{ venv_dir }}/bin/tutor config save --set KUBERNETES_STORAGE_CLASS=longhorn"
      register: storage_config
      changed_when: storage_config.rc == 0
      
    - name: Configure optimized storage classes for databases
      ansible.builtin.shell: |
        {{ venv_dir }}/bin/tutor config save --set KUBERNETES_PVC_STORAGE_CLASS_MYSQL="longhorn-db-performance"
        {{ venv_dir }}/bin/tutor config save --set KUBERNETES_PVC_STORAGE_CLASS_MONGODB="longhorn-db-performance"
        {{ venv_dir }}/bin/tutor config save --set KUBERNETES_PVC_STORAGE_CLASS_ELASTICSEARCH="longhorn-db-performance"
      changed_when: true
      when: "'not found' not in db_sc_check.stdout"
      
    - name: Display storage configuration message
      debug:
        msg: |
          Configured Tutor to use Longhorn storage for high availability.
          - Default storage class: longhorn
          - Database storage class: {{ 'longhorn-db-performance' if 'not found' not in db_sc_check.stdout else 'longhorn' }}
  when: longhorn_sc.rc == 0 and longhorn_sc.stdout_lines | length > 0

- name: Configure Tutor with proper hostnames and settings
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor config save --set LMS_HOST={{ lms_host }} --set CMS_HOST={{ cms_host }}"
  register: tutor_config

- name: Configure resource limits for Kubernetes deployment
  ansible.builtin.shell: |
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_LMS_LIMITS_CPU={{ resource_limits.lms.cpu }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_LMS_LIMITS_MEMORY={{ resource_limits.lms.memory }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_CMS_LIMITS_CPU={{ resource_limits.cms.cpu }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_CMS_LIMITS_MEMORY={{ resource_limits.cms.memory }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_MYSQL_LIMITS_CPU={{ resource_limits.mysql.cpu }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_MYSQL_LIMITS_MEMORY={{ resource_limits.mysql.memory }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_MONGODB_LIMITS_CPU={{ resource_limits.mongodb.cpu }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_MONGODB_LIMITS_MEMORY={{ resource_limits.mongodb.memory }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_ELASTICSEARCH_LIMITS_CPU={{ resource_limits.elasticsearch.cpu }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_ELASTICSEARCH_LIMITS_MEMORY={{ resource_limits.elasticsearch.memory }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_REDIS_LIMITS_CPU={{ resource_limits.redis.cpu }}
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_REDIS_LIMITS_MEMORY={{ resource_limits.redis.memory }}
  register: resource_config

- name: Save Tutor configuration
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor config save"
  changed_when: true

- name: Create tutor deployment wrapper script
  ansible.builtin.template:
    src: tutor-deploy-wrapper.sh.j2
    dest: /tmp/tutor-deploy-wrapper.sh
    mode: '0755'

- name: Create resource reducer script
  ansible.builtin.template:
    src: resource-reducer.yaml.j2
    dest: /tmp/resource-reducer.yaml
    mode: '0644'

- name: Execute resource reducer
  ansible.builtin.shell: |
    # Apply and run the resource reducer
    kubectl apply -f /tmp/resource-reducer.yaml
    
    # Directly apply resource reductions
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_REQUESTS_ENABLED=false
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_LIMITS_ENABLED=true
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_LMS_LIMITS_CPU=1
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_LMS_LIMITS_MEMORY=1Gi
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_CMS_LIMITS_CPU=1
    {{ venv_dir }}/bin/tutor config save --set K8S_RESOURCES_CMS_LIMITS_MEMORY=1Gi
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
  register: resource_reduction
  changed_when: resource_reduction.rc == 0

- name: Deploy Open edX on Kubernetes using resilient wrapper
  ansible.builtin.shell: /tmp/tutor-deploy-wrapper.sh
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
  register: deploy_result
  changed_when: true

- name: Get detailed pod status
  debug:
    msg: "=== Pod Status ===\n{{ lookup('pipe', 'kubectl get pods -n {{ openedx_namespace }} -o wide') }}"

- name: Check pod events
  debug:
    msg: "=== Pod Events ===\n{{ lookup('pipe', 'kubectl get events -n {{ openedx_namespace }}') }}"

- name: Get pod descriptions
  debug:
    msg: "=== Pod Descriptions ===\n{{ lookup('pipe', 'kubectl describe pods -n {{ openedx_namespace }}') }}"

- name: Get list of pods
  command: kubectl get pods -n {{ openedx_namespace }} -o json
  register: pods_json
  changed_when: false

- name: Get container logs for each pod
  command: kubectl logs -n {{ openedx_namespace }} {{ item }}
  loop: "{{ pods_json.stdout | from_json | json_query('items[*].metadata.name') }}"
  loop_control:
    label: "{{ item }}"
  register: pod_logs
  changed_when: false
  ignore_errors: true

# Add node recovery and PVC remedy solutions for node unreachable issues
- name: Create node recovery template file
  ansible.builtin.template:
    src: node-recovery.yaml.j2
    dest: /tmp/node-recovery.yaml

- name: Create PVC remedy template file
  ansible.builtin.template:
    src: pvc-remedy.yaml.j2
    dest: /tmp/pvc-remedy.yaml

- name: Apply node recovery daemonset
  ansible.builtin.shell: |
    kubectl apply -f /tmp/node-recovery.yaml
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
  register: node_recovery_result
  changed_when: node_recovery_result.rc == 0
  failed_when: false

- name: Display node recovery status
  debug:
    msg: "Node recovery DaemonSet applied: {{ node_recovery_result.stdout }}"

- name: Apply PVC remedy job
  ansible.builtin.shell: |
    kubectl apply -f /tmp/pvc-remedy.yaml
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
  register: pvc_remedy_result
  changed_when: pvc_remedy_result.rc == 0
  failed_when: false

- name: Display PVC remedy status
  debug:
    msg: "PVC remedy resources applied: {{ pvc_remedy_result.stdout }}"

- name: Wait for pods to be ready
  block:
    - name: Set retry count
      ansible.builtin.set_fact:
        retry_count: 0
        all_ready: false
        max_retries: 45  # Increased from default 30 for better reliability
        
    - name: Check pod status with retry
      ansible.builtin.shell: |
        # Get pod status
        kubectl get pods -n {{ openedx_namespace }}
        
        # Check for ImagePullBackOff issues
        IMAGE_PULL_ISSUES=$(kubectl get pods -n {{ openedx_namespace }} -o json | jq -r '.items[] | select(.status.phase != "Running" and .status.containerStatuses[0].state.waiting.reason == "ImagePullBackOff") | .metadata.name')
        
        if [ ! -z "$IMAGE_PULL_ISSUES" ]; then
          echo "Detected ImagePullBackOff issues. Attempting to fix..."
          for pod in $IMAGE_PULL_ISSUES; do
            echo "Deleting pod $pod to trigger a retry..."
            kubectl delete pod -n {{ openedx_namespace }} $pod
          done
          echo "Waiting 30 seconds for new pods to be created..."
          sleep 30
        fi
        
        # Check for stuck Terminating pods
        STUCK_PODS=$(kubectl get pods -n {{ openedx_namespace }} | grep Terminating | awk '{print $1}')
        if [ ! -z "$STUCK_PODS" ]; then
          echo "Detected pods stuck in Terminating state. Force deleting them..."
          for pod in $STUCK_PODS; do
            echo "Force deleting pod: $pod"
            kubectl delete pod -n {{ openedx_namespace }} $pod --force --grace-period=0
          done
          echo "Waiting 10 seconds for cleanup..."
          sleep 10
        fi
        
        # Check if all pods are ready
        NOT_READY=$(kubectl get pods -n {{ openedx_namespace }} -o json | jq -r '.items[] | select(.status.phase != "Running" and .status.phase != "ContainerCreating" or ([ .status.containerStatuses[] | select(.ready == false) ] | length > 0)) | .metadata.name')
        
        # Count pods in ContainerCreating state
        CONTAINER_CREATING=$(kubectl get pods -n {{ openedx_namespace }} -o json | jq -r '.items[] | select(.status.phase == "ContainerCreating") | .metadata.name' | wc -l)
        
        if [ -z "$NOT_READY" ]; then
          echo "all_ready=true"
          echo "container_creating=0"
          exit 0
        else
          echo "all_ready=false"
          echo "container_creating=$CONTAINER_CREATING"
          exit 1
        fi
      register: pod_status
      retries: "{{ max_retries | default(30) }}"
      delay: 30
      until: pod_status.stdout_lines[-1] == "all_ready=true" or pod_status.stdout_lines[-2] == "container_creating=0"
      
    - name: Display pod status
      debug:
        msg: "Pod status: {{ pod_status.stdout }}"
      when: pod_status is defined

    - name: Check for persistent ImagePullBackOff issues
      ansible.builtin.shell: |
        kubectl get pods -n {{ openedx_namespace }} -o json | jq -r '.items[] | select(.status.phase != "Running" and .status.containerStatuses[0].state.waiting.reason == "ImagePullBackOff") | .metadata.name'
      register: persistent_image_issues
      
    - name: Display warning if there are persistent image pull issues
      debug:
        msg: |
          Warning: Some pods are stuck in ImagePullBackOff state:
          {{ persistent_image_issues.stdout }}
          This might be due to:
          1. Docker Hub rate limits
          2. Network connectivity issues
          3. Insufficient file descriptor limits
          
          You can try:
          1. Running 'make registry-auth' to reconfigure Docker registry authentication
          2. Increasing file descriptor limits (see README for instructions)
          3. Using a Docker Hub account with higher rate limits
      when: persistent_image_issues.stdout != ""

    - name: Initialize Open edX
      ansible.builtin.command: "{{ venv_dir }}/bin/tutor k8s start"
      when: pod_status.stdout_lines[-1] == "all_ready=true"
      changed_when: true

- name: Initialize Open edX
  ansible.builtin.command: "{{ venv_dir }}/bin/tutor k8s init"
  ignore_errors: true
  changed_when: true

- name: Check if Longhorn is being used
  ansible.builtin.shell: |
    kubectl get pvc -n {{ openedx_namespace }} -o json | jq -r '.items[] | select(.spec.storageClassName | contains("longhorn")) | .metadata.name' || echo ""
  register: longhorn_pvcs
  ignore_errors: true
  changed_when: false

- name: Display high-availability storage status
  debug:
    msg: |
      Your OpenedX instance is using Longhorn high-availability storage!
      PVCs using Longhorn: {{ longhorn_pvcs.stdout_lines | join(', ') }}
      
      Benefits:
      - Data is replicated across multiple nodes
      - Pods can be rescheduled to any node if a node fails
      - Automatic volume healing if a node recovers
  when: longhorn_pvcs.stdout_lines | length > 0

- name: Display deployment information
  debug:
    msg: |
      To access your Open edX instance, you need to add the following entries to your /etc/hosts file:
      127.0.0.1 {{ lms_host }} {{ cms_host }}
      
      To create a superuser account, run:
      {{ venv_dir }}/bin/tutor k8s exec lms -- python manage.py lms createsuperuser
      
      To check the status of your deployment:
      {{ venv_dir }}/bin/tutor k8s status
      
      To port-forward the LMS service to access it locally:
      kubectl port-forward -n {{ openedx_namespace }} svc/lms 8000:8000
      Then access: http://{{ lms_host }}:8000
      
      To port-forward the Studio service to access it locally:
      kubectl port-forward -n {{ openedx_namespace }} svc/cms 8001:8000
      Then access: http://{{ cms_host }}:8001